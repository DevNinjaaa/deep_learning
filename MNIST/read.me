MNIST Handwritten Digit Recognition using PyTorch

This project implements a Convolutional Neural Network (CNN) to classify handwritten digits from the MNIST dataset using PyTorch. The model is designed to achieve high accuracy while providing a basic understanding of deep learning concepts and PyTorch functionality.

Table of Contents
- Introduction
- Installation
- Usage
- Model Architecture
- Training
- Results
- Conclusion
- License

Introduction

The MNIST dataset contains 70,000 images of handwritten digits (0-9) in grayscale. This project aims to build a neural network that accurately classifies these digits using a CNN architecture. 

Installation

To set up this project, you need to have Python and pip installed. Follow these steps:

1. Clone the repository:
   git clone https://github.com/DevNinjaaa/deep_learning.git
   cd deep_learning/mnist

2. Create a virtual environment (optional but recommended):
   python -m venv venv
   source venv/bin/activate  # On Windows use `venv\Scripts\activate`

3. Install the required packages:
   pip install -r requirements.txt

Usage

To run the training and testing of the model, execute the following command:
python app.py

The model will train on the MNIST dataset, and training progress will be displayed in the terminal. 

Model Architecture

The CNN architecture consists of three convolutional layers followed by max pooling and dropout for regularization. The final fully connected layers output the probabilities for each digit class (0-9).

- Convolutional Layers: Extract features from the input images.
- Max Pooling: Reduce the spatial dimensions of the feature maps.
- Dropout: Prevent overfitting during training.
- Fully Connected Layers: Classify the extracted features into digit classes.

Here is a high-level view of the architecture:

Input (28x28) -> Conv Layer 1 -> ReLU -> Max Pool -> Conv Layer 2 -> ReLU -> Max Pool -> Conv Layer 3 -> ReLU -> Flatten -> FC Layer -> Output (10 classes)

Training

The model is trained for a specified number of epochs, with a batch size of 100. The training process includes monitoring the loss and accuracy metrics. Early stopping can be implemented to prevent overfitting if the validation loss stops improving.

Training Parameters
- Epochs: 1-100 (adjustable in the train function)
- Batch Size: 100
- Optimizer: Adam
- Loss Function: Cross Entropy Loss

Results

After training, the model achieves an accuracy of approximately 95% on the test set. 

You can visualize the results and predicted classes by modifying the app.py script to display test images and their corresponding predictions.

Conclusion

This project serves as an introduction to convolutional neural networks and their application to image classification tasks. The techniques learned here can be expanded to more complex datasets and deeper models.

License

This project is licensed under the MIT License - see the LICENSE file for details.
